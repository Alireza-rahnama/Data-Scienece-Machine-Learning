# -*- coding: utf-8 -*-
"""ml_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QMH1wC4vbArQmLlpq0weKlt6p3_xgDgl
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
# import seaborn as sns
import re
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import neighbors, datasets, preprocessing
import statsmodels.api as sm

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import scale
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV
from sklearn.linear_model import *
import sys

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import make_pipeline

from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
from sklearn.metrics import r2_score

from google.colab import drive
drive.mount('/content/drive')

file_dir_list=["/content/drive/MyDrive/Torque and ROP Modeling Data/17.5in Drilling.csv","/content/drive/MyDrive/Torque and ROP Modeling Data/12.25in Drilling.csv","/content/drive/MyDrive/Torque and ROP Modeling Data/8.5in Drilling.csv"]
# tsv_file2=sys.argv[2]

dataframe_list=[pd.read_csv(i) for i in file_dir_list ]
data = pd.concat(dataframe_list)
if "Unnamed: 0" in data.columns:
  data.drop("Unnamed: 0",axis=1)
if "TIME" in data.columns:
  data=data.drop('TIME',axis=1)


train=data.sample(frac=0.8,random_state=200)
test=data.drop(train.index)

featuresX=train[[col for col in data.columns if((col != "ROP (m/h)"))]]
classY=train[["ROP (m/h)"]]
X_train, X_test, y_train, y_test = train_test_split(featuresX, classY, test_size=0.25)
# print(classY)

train.head()

print("train:",train.shape)
print("test:",test.shape)
print("data shape: ",data.shape)

def evaluate(model, test_features, test_labels):
    predictions = model.predict(test_features)
    errors = abs(predictions - test_labels)
    mape = 100 * np.mean(errors / test_labels)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f}'.format(np.mean(errors)))
    print('Accuracy = {:0.4f}%.'.format(accuracy))
    
    return accuracy

features_test_x=test[[col for col in data.columns if((col != "ROP (m/h)"))]]
class_test_y=test[["ROP (m/h)"]]

from pandas.core.common import random_state
folds = KFold(n_splits = 3, shuffle = True, random_state = 100)

# step-2: specify range of hyperparameters to tune
hyper_params = [{'n_features_to_select': [55]}]


# step-3: perform grid search
# 3.1 specify model
lm = LinearRegression()
rfe = RFE(lm)             

# 3.2 call GridSearchCV()
model_cv = GridSearchCV(estimator = rfe, 
                        param_grid = hyper_params, 
                        scoring= 'r2', 
                        cv = folds, 
                        verbose = 1,
                        return_train_score=False)  


lr= model_cv.fit(X_train,y_train)
y_predict=lr.predict(X_test)
print("The coefficient of determination(r squared) obtained from Linear Regression:\n")
######score here returns The coefficient of determination(r squared) the closer to 1 the better model
print(lr.score(X_train,y_train),"\n") 
##print(pd.DataFrame(model_cv.cv_results_))
print("best model:",model_cv.best_estimator_)
print("best parameters: ",model_cv. best_params_)
print("\n")

# evaluate(lr,features_test_x,class_test_y.values.flatten())
y_predicted = lr.predict(features_test_x)
predicted=pd.DataFrame((y_predicted),columns=["ROP Predicted"])



import matplotlib.pyplot as plt
import numpy as np


fig = plt.figure() 

fig.set_size_inches(14,12) 

y1 = class_test_y.sort_index(axis=1)
print(y1.shape)


y2 = predicted[["ROP Predicted"]].sort_index(axis=1)
print(y2.shape)


plt.plot(y1, y1, color='green', linewidth=2,linestyle='-')
plt.scatter(y1, y2, marker='o', linewidth=2,linestyle='-') 
# naming the x axis
plt.xlabel('Actual ROP')
# naming the y axis
plt.ylabel('Predicted ROP')


plt.title('Linear Regression Predicted ROP Vs. Actual ROP (m/h)')




plt.legend()

plt.show()

!pip install dataprep
from dataprep.eda import create_report

import sys
import os
#change the directory wher you want the EDA report saved
output_directory='/content/drive/MyDrive/Colab Notebooks/reports/'
if not os.path.exists(output_directory):
  os.makedirs(output_directory)
report=create_report(data)
report.save(output_directory+'all_sections_combined_with_drilling_status.html')

"""The most popular resampling technique to metigate overfitting is k-fold cross validation. It allows you to train and test your model k-times on different subsets of training data and build up an estimate of the performance of a machine learning model on unseen data.

"""

from sklearn.ensemble import RandomForestRegressor
param_grid = {
    'n_estimators': [10]
    # 'max_depth': [int(x) for x in np.linspace(10, 110, num = 21)]
             }
# # step-1: create a cross-validation scheme
folds = KFold(n_splits = 3, shuffle = True, random_state = 10)
regr = RandomForestRegressor(random_state = 10)
# # rfe_random_forest = RFE(regr,n_features_to_select=10)             

m3_cv = GridSearchCV(estimator = regr,
                     param_grid = param_grid,
                     scoring= 'r2',
                     cv=folds,
                     return_train_score=False,
                     verbose= 1,
                     n_jobs=-1)  


forest_model= m3_cv.fit(X_train,y_train.values.flatten())


print(f"The coefficient of determination(r squared) obtained from RandomForestRegressor: {forest_model.score(X_train,y_train)}\n")
######score here returns The coefficient of determination(r squared) the closer to 1 the better model
# print(m3_cv.score(X_train,y_train.values.faltten()),"\n") 
##print(pd.DataFrame(m3_cv.cv_results_))
print("best model:",forest_model.best_estimator_)
print("best parameters: ",forest_model.best_params_)
print("\n")

evaluate(forest_model,features_test_x,class_test_y.values.flatten())

#predict the y values

y_predicted = forest_model.predict(features_test_x)
predicted=pd.DataFrame((y_predicted),columns=["ROP Predicted"])


fig = plt.figure() 

fig.set_size_inches(14,12) 
# # line 1 points
y1 = class_test_y.sort_index(axis=1)
print(y1.shape)
# # # plotting the line 1 points 

y2 = predicted[["ROP Predicted"]].sort_index(axis=1)
print(y2.shape)
# plotting the line 2 points 
# plt.scatter(x2, y2, label = "Predicted Data", color='red', marker='.')

plt.plot(y1, y1, color='green', linewidth=2,linestyle='-')
plt.scatter(y1, y2, marker='o', linewidth=2,linestyle='-') 
# naming the x axis
plt.xlabel('Actual ROP')
# naming the y axis
plt.ylabel('Predicted ROP')


plt.title('Random Forest Predicted ROP Vs. Actual ROP (m/h)')

# show a legend on the plot
plt.legend()

# function to show the plot
plt.show()

from numpy import asarray
from pandas import read_csv
from xgboost import XGBRegressor
# load the dataset

# define model
model = XGBRegressor()
# fit model
model.fit(X_train, y_train)
# define new data
# new_data = asarray([row])
# make a prediction
yhat = model.predict(X_test)
# summarize prediction

print("the coefficient determination  of XGBoost is: ", model.score(X_train, y_train))

evaluate(model,features_test_x,class_test_y.values.flatten())

#predict the y values

y_predicted = model.predict(features_test_x)
predicted=pd.DataFrame((y_predicted),columns=["ROP Predicted"])


fig = plt.figure() 

fig.set_size_inches(14,12) 
# # line 1 points
y1 = class_test_y.sort_index(axis=1)
print(y1.shape)
# # # plotting the line 1 points 

y2 = predicted[["ROP Predicted"]].sort_index(axis=1)
print(y2.shape)
# plotting the line 2 points 
# plt.scatter(x2, y2, label = "Predicted Data", color='red', marker='.')

plt.plot(y1, y1, color='green', linewidth=2,linestyle='-')
plt.scatter(y1, y2, marker='o', linewidth=2,linestyle='-') 
# naming the x axis
plt.xlabel('Actual ROP')
# naming the y axis
plt.ylabel('Predicted ROP')


plt.title('XGBoost Predicted ROP Vs. Actual ROP (m/h)')

# show a legend on the plot
plt.legend()

# function to show the plot
plt.show()

"""But overfitting should not be confused with model selection.

We choose a predictive model or model configuration based on its out-of-sample performance. That is, its performance on new data not seen during training.

"""